{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "from utils import cutout\n",
    "\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "horse   dog  deer   dog plane  deer  frog   car  frog   dog  bird  bird   dog  frog   cat   dog\n"
     ]
    }
   ],
   "source": [
    "# functions to show an image\n",
    "\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "## cifar-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform= transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imgs = []\n",
    "# show images\n",
    "for i in range(len(images)):\n",
    "\n",
    "    img = (images[i]/2 +.5)*100  # unnormalize\n",
    "    im = img.cpu().detach().numpy()\n",
    "    # im = img.numpy()\n",
    "    im = np.transpose(im, (1,2,0))\n",
    "    im = np.uint8(im)\n",
    "    imgs.append(Image.fromarray(im, 'RGB')) \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x128 at 0x7FC6570CE7C0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = []\n",
    "# show images\n",
    "for i in range(len(images)):\n",
    "\n",
    "    img = (images[i]/2 +.5)*100  # unnormalize\n",
    "    im = img.cpu().detach().numpy()\n",
    "    im = np.transpose(im, (1,2,0))\n",
    "    im = np.uint8(im)\n",
    "    im = Image.fromarray(im, 'RGB')\n",
    "    im = im.resize((128,128))\n",
    "    im_draw = ImageDraw.Draw(im)\n",
    "    \n",
    "    gt = \"\\nGT:\"\n",
    "    pd = \"Pred:\"\n",
    "    tv = classes[labels[0]]\n",
    "    text = pd + gt + tv\n",
    "    fontsize = 1\n",
    "    im_draw.text((0,0),text)\n",
    "\n",
    "    imgs.append(im)\n",
    "\n",
    "\n",
    "imgs[0]\n",
    "\n",
    "# collage = Image.new('RGB', (32, 32*len(imgs)))\n",
    "# for i in range(len(imgs)):\n",
    "#     collage.paste(imgs[i], (0,32*i))\n",
    "#     collage.save('test.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (images[0]/2 +.5)*100  # unnormalize\n",
    "im = img.cpu().detach().numpy()\n",
    "# im = img.numpy()\n",
    "im = np.transpose(im, (1,2,0))\n",
    "im = np.uint8(im)\n",
    "im = Image.fromarray(im, 'RGB')\n",
    "im = im.resize((128,128))\n",
    "\n",
    "im_draw = ImageDraw.Draw(im)\n",
    "gt = \"\\nGT:\"\n",
    "pd = \"Pred:\"\n",
    "tv = classes[labels[0]]\n",
    "text = pd + gt + tv\n",
    "fontsize = 1\n",
    "# imgs.append(Image.fromarray(im, 'RGB'))\n",
    "\n",
    "im_draw.text((0,0),text)\n",
    "\n",
    "im.save('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement DenseNet architecture into image classification tutorial\n",
    "class Dense_Layer(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Dense_Layer, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(n_in)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv(self.relu(self.bn(x)))\n",
    "        out = torch.cat([out,x],1)\n",
    "        return out\n",
    "\n",
    "class Transition_Layer(nn.Sequential):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Transition_Layer, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(n_in)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(n_in, n_out,kernel_size=1,stride=1,padding = 0,bias=0)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv(self.relu(self.bn(x))))\n",
    "        return x\n",
    "\n",
    "class Dense_Block(nn.Module):\n",
    "    def __init__(self, n_in, growth):\n",
    "        super(Dense_Block, self).__init__()\n",
    "        self.layer = self.dense_block(n_in, growth)\n",
    "    def dense_block(self, n_in, growth):\n",
    "        layers = []\n",
    "        for i in range(0,4):\n",
    "            layers.append(Dense_Layer(n_in + i*growth, growth))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class DenseNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet3, self).__init__()\n",
    "        #initial convolution\n",
    "        self.conv0 = nn.Conv2d(3, 6, kernel_size=3, stride = 1, padding=1, bias=False)\n",
    "        \n",
    "        #first denseblock\n",
    "        self.dense1 = Dense_Block(n_in = 6, growth = 16)\n",
    "        #note since there are 4 layers the output of denseblock will have 6 +4*16 channels\n",
    "        self.trans1 = Transition_Layer(n_in = 70, n_out = 35)\n",
    "\n",
    "        #second denseblock\n",
    "        self.dense2 = Dense_Block(n_in = 35, growth = 16)\n",
    "        #note since there are 4 layers the output of denseblock will have 35 + 4*16\n",
    "        self.trans2 = Transition_Layer(n_in = 99, n_out=50)\n",
    "\n",
    "        #third denseblock\n",
    "        self.dense3 = Dense_Block(n_in=50,growth=16)\n",
    "\n",
    "        #global average pooling, classifier\n",
    "        self.bnend =  nn.BatchNorm2d(114)\n",
    "        self.classifier = nn.Linear(114, 10)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.Linear):\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = F.relu(self.bnend(out))\n",
    "        out = F.adaptive_avg_pool2d(out, (1,1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4008/3066929338.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mnew_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_ls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2979\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2980\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2981\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2982\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "#print cutout images\n",
    "image_ls = []\n",
    "\n",
    "for i,x in enumerate(images):\n",
    "\n",
    "    new_image = cutout(1,32,x)\n",
    "\n",
    "    image_ls.append(new_image)\n",
    "    \n",
    "\n",
    "new_images = torch.stack(image_ls)\n",
    "\n",
    "ims = Image.getIm(torchvision.utils.make_grid(new_images))\n",
    "\n",
    "ims.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Model Architecture:\n",
      "\n",
      " DenseNet3(\n",
      "  (conv0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Dense_Block(\n",
      "    (layer): Sequential(\n",
      "      (0): Dense_Layer(\n",
      "        (bn): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): Dense_Layer(\n",
      "        (bn): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(22, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): Dense_Layer(\n",
      "        (bn): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(38, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): Dense_Layer(\n",
      "        (bn): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(54, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition_Layer(\n",
      "    (bn): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv): Conv2d(70, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (dense2): Dense_Block(\n",
      "    (layer): Sequential(\n",
      "      (0): Dense_Layer(\n",
      "        (bn): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(35, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): Dense_Layer(\n",
      "        (bn): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(51, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): Dense_Layer(\n",
      "        (bn): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(67, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): Dense_Layer(\n",
      "        (bn): BatchNorm2d(83, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(83, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition_Layer(\n",
      "    (bn): BatchNorm2d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv): Conv2d(99, 50, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  )\n",
      "  (dense3): Dense_Block(\n",
      "    (layer): Sequential(\n",
      "      (0): Dense_Layer(\n",
      "        (bn): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(50, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): Dense_Layer(\n",
      "        (bn): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(66, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (2): Dense_Layer(\n",
      "        (bn): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(82, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (3): Dense_Layer(\n",
      "        (bn): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv): Conv2d(98, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bnend): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Linear(in_features=114, out_features=10, bias=True)\n",
      ")\n",
      "Starting Epoch 1\n",
      "Train Loss: 1.874 | Train Accuracy: 31.172 %\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (40) to match target batch_size (20).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4008/1638680694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    962\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/comp0090-cw1/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0m\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (40) to match target batch_size (20)."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ## cifar-10 dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 40\n",
    "\n",
    "    #get training set\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    #get test set\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    #declare classes\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "    ## densenet\n",
    "    net = DenseNet3()\n",
    "    ##print model architexture\n",
    "    print(\"Model Architecture:\")\n",
    "    print(\"\\n\",net)\n",
    "\n",
    "    #define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=.001, momentum =.9)\n",
    "\n",
    "    #initialize empty vectors to store values\n",
    "    train_accuracy = []\n",
    "    train_losses =  []\n",
    "    test_accuracy = []\n",
    "    test_losses = []\n",
    "\n",
    "    ## train\n",
    "    for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "        print(f'Starting Epoch {epoch+1}')\n",
    "\n",
    "        #train\n",
    "        running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            #insert cutout algorithm into training\n",
    "            cutouts = []\n",
    "            for j,x in enumerate(inputs):\n",
    "                new_image = cutout(1,32,x)\n",
    "                cutouts.append(new_image)\n",
    "            \n",
    "            inputs = torch.stack(cutouts)\n",
    "\n",
    "            #save cutout image for first batch\n",
    "            if(epoch == 0):\n",
    "                if (i == 0):\n",
    "\n",
    "                    imgs = []\n",
    "                    # get images\n",
    "                    for i in range(len(inputs)):\n",
    "\n",
    "                        img = (inputs[i]/2 +.5)*100  # unnormalize\n",
    "                        im = img.cpu().detach().numpy()\n",
    "                        # im = img.numpy()\n",
    "                        im = np.transpose(im, (1,2,0))\n",
    "                        im = np.uint8(im)\n",
    "                        imgs.append(Image.fromarray(im, 'RGB'))\n",
    "\n",
    "                    #create collage\n",
    "                    collage = Image.new('RGB', (32, 32*16))\n",
    "                    for i in range(16):\n",
    "                        collage.paste(imgs[i], (0,32*i))\n",
    "                        collage.save('cutout.png')\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _,predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        \n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        train_accu = 100.*correct/total\n",
    "\n",
    "        print('Train Loss: %.3f | Train Accuracy: %.3f'%(train_loss,train_accu),'%')\n",
    "\n",
    "        #test\n",
    "        running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0\n",
    "\n",
    "        #forward pass only\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(testloader,0):\n",
    "                inputs, lables = data\n",
    "\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs,labels)\n",
    "                running_loss+=loss.item()\n",
    "\n",
    "                _,predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct +=predicted.eq(labels).sum().item()\n",
    "\n",
    "                #save a png of results \n",
    "                if (epoch == 10):\n",
    "                    if (i == 0):\n",
    "                        imgs = []\n",
    "                        # get images\n",
    "                        for i in range(len(inputs)):\n",
    "\n",
    "                            img = (images[i]/2 +.5)*100  # unnormalize\n",
    "                            im = img.cpu().detach().numpy()\n",
    "                            im = np.transpose(im, (1,2,0))\n",
    "                            im = np.uint8(im)\n",
    "                            im = Image.fromarray(im, 'RGB')\n",
    "                            im = im.resize((128,128))\n",
    "                            im_draw = ImageDraw.Draw(im)\n",
    "    \n",
    "                            gt = \"\\nGT:\"\n",
    "                            pd = \"Pred:\"\n",
    "                            tv = classes[labels[0]]\n",
    "                            yh = classes[predicted[0]]\n",
    "                            text = pd + yh + gt + tv\n",
    "                            fontsize = 1\n",
    "                            im_draw.text((0,0),text)\n",
    "\n",
    "                            imgs.append(im)\n",
    "\n",
    "                        #create collage\n",
    "                        collage = Image.new('RGB', (128, 128*36))\n",
    "                        for i in range(36):\n",
    "                            collage.paste(imgs[i], (0,128*i))\n",
    "                            collage.save('results.png')\n",
    "\n",
    "        test_loss = running_loss/len(testloader)\n",
    "        test_accu = 100.*correct/total\n",
    "\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracy.append(test_accu)\n",
    "\n",
    "        print('Test Loss: %.3f | Test Accuracy: %.3f'%(test_loss,test_accu),'%')\n",
    "                    \n",
    "\n",
    "    print('Training done.')\n",
    "\n",
    "    # save trained model\n",
    "    torch.save(net.state_dict(), 'saved_model.pt')\n",
    "    print('Model saved.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5afa7924572cd6c7a930d7ef95861535e4abe76324ebec577ecc14b634e14ac8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('comp0090-cw1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
